{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMFpVsouQ9Fibx2UacvXpUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardo1980/colab_bench/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDhDs0QAG8bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygFaK8tzHtTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e8fa9e4-e798-4487-df29-2b63d7f1d83a"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049Pf0JvHyKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALAwgdDFKnii",
        "colab_type": "text"
      },
      "source": [
        "# Dataset download\n",
        "Resources: http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUjrFBtZhDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4129c557-e174-433c-da1e-70e85ddeed9e"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGO6Q_ODZs0b",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHh28tNZj3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s: str) -> str:\n",
        "  \"\"\"\n",
        "    Converts unicode string to ascii\n",
        "  \n",
        "    Non-spacing marks (Mn category) are discarded,\n",
        "    see https://www.fileformat.info/info/unicode/category/Mn/list.htm \n",
        "\n",
        "    Applies Normalization Form C (NFC)\n",
        "\n",
        "    :param s: unicode string\n",
        "    :return: ascii string\n",
        "  \"\"\"\n",
        "\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w: str) -> str:\n",
        "  \"\"\"\n",
        "    Convert single string sentence\n",
        "    \n",
        "    1. unicode to ascii\n",
        "    2. adds a space between word and following punctuation\n",
        "    3. removes all chars except a-Z, A-Z, , \".\", \"?\", \"!\", \",\",\"¿\"\n",
        "    4. removes leading/trailing blanks\n",
        "    5. adds start/end tokens\n",
        "\n",
        "    :param w: unicode string string\n",
        "    :return: cleaned string  \n",
        "  \"\"\"\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except basic punctuation and alpha chars\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DAaLVbhaLNY",
        "colab_type": "text"
      },
      "source": [
        "## Test preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLFUh9IZaG4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8d695972-524d-4e01-c5e8-dde2d7e892a5"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyF2p648aokj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGND9F-baPoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "def create_dataset(path: str, num_examples: int) -> List[List[str]]:\n",
        "  \"\"\"\n",
        "    Create pairs of sentences\n",
        "  \n",
        "    1. Remove the accents\n",
        "    2. Clean the sentences\n",
        "    3. Return sentences grouped by language]\n",
        "\n",
        "    :param path: path to input file\n",
        "    :param num_examples: maximum number of examples\n",
        "    :return: tuple containing two list of sequences, one for each column in input file \n",
        "  \"\"\"\n",
        "  # each line contains two columns separated by tab character\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  # split lines, preprocess phrases, get a tuple for each line\n",
        "  sentence_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  # rearrange to a tuple for each language\n",
        "  return zip(*sentence_pairs)\n",
        "\n",
        "\n",
        "def tokenize(lang: List[str]) -> Tuple[np.ndarray, tf.keras.preprocessing.text.Tokenizer]:\n",
        "  \"\"\"\n",
        "    Fit a tokenizer on input list of sentences\n",
        "\n",
        "    From words (string) to symbols (integer)\n",
        "\n",
        "    :param lang: list of sentences of same language\n",
        "    :return: a tuple of:\n",
        "      a tensor of size [NUMBER_OF_SENTENCES, SENTENCE_SIZE] contaning the vectorizations of the sentences\n",
        "      a learned tokenizer\n",
        "  \"\"\"\n",
        "\n",
        "  # create vanilla tokenizer\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  \n",
        "  # learn tokenization procedure on given set of sentences\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  # transforms sentences in sequences of integers (sequences are of different lengths)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  # pad sequenes\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None) -> Tuple[np.ndarray, np.ndarray, tf.keras.preprocessing.text.Tokenizer, tf.keras.preprocessing.text.Tokenizer]:\n",
        "  \"\"\"\n",
        "    Load dataset, with preprocessing and tokenization\n",
        "\n",
        "    :param path: path to input file\n",
        "     :param num_examples: maximum number of examples\n",
        "  \"\"\"\n",
        "\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  # tokenization\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToWcx0ja4DJ",
        "colab_type": "text"
      },
      "source": [
        "## Test dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ0ASMxta0ob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "93f78e72-a237-4eb7-9a80-000ba35ff620"
      },
      "source": [
        "target_sentences, input_sentences = create_dataset(path_to_file, None)\n",
        "print(target_sentences[-1])\n",
        "print(input_sentences[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UphyWTcMOAbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns-I_sQWQg_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eae73ca1-ecb8-4beb-ff60-c0d95e384b80"
      },
      "source": [
        "target_sentences[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> go . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOABTxqfTgdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "191dfd6f-8eee-4f6c-c226-07f058a311f0"
      },
      "source": [
        "targ_lang_tokenizer.texts_to_sequences([['go']])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[36]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61GBP7nzTuC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dad48da5-50c1-4b80-dcd2-63ceb5266a47"
      },
      "source": [
        "targ_lang_tokenizer.sequences_to_texts(\n",
        "    [np.arange(20)]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> <end> . i tom you ? is a it s t the he to we me m this']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt89-PaVU6wL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31854938-7f5b-4afe-dc0b-c2d83a7e7de0"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print((max_length_targ, max_length_inp))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYaiUQSVd1g",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzvsBrm0Vctw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 30000\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "purU06h-VGLH",
        "colab_type": "text"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqXhEg8_U_3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, num_examples)\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnHdHnaVVL1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp1QdXWKWFAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDkyjz2IId_p",
        "colab_type": "text"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ixFoEnIfk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size: int, \n",
        "               embedding_dim: int, units: int, batch_size: int):\n",
        "    \"\"\"\n",
        "      Initialize the encoder\n",
        "\n",
        "      :param vocab_size: size of the vocabulary, i.e. maximum integer index + 1.\n",
        "      :param embedding_dim: dimension of the dense embedding\n",
        "      :param units: dimensionality of the output space.\n",
        "      :param batch_size: batch size\n",
        "\n",
        "    \"\"\"\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.units = units\n",
        "\n",
        "    # initialize embedding layer (from symbols to dense vectors)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # initialize recurrent cells\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "  def call(self, x: tf.Tensor, hidden: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"\n",
        "      Forward pass\n",
        "\n",
        "      :param x: input tensor of shape (batch_size, max_input_length)\n",
        "      :param hidden: hidden state initialization tensor of shape (batch_size, units)\n",
        "      \n",
        "      :return:\n",
        "        output tensor of shape (batch_size, sequence_length, units)\n",
        "        state tensor of shape (batch_size, units)\n",
        "    \"\"\"\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "      Initialize hidden state\n",
        "\n",
        "      :return: a tensor of shape (batch_size, units)\n",
        "    \"\"\"\n",
        "    return tf.zeros((self.batch_size, self.units))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEKuvfsEB1L",
        "colab_type": "text"
      },
      "source": [
        "## Test encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzduQHqULBmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcfdNe4tMCg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "469e1ccf-fe31-4e6e-bce2-d1347a156735"
      },
      "source": [
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "# sample input\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "print((example_input_batch.shape, example_target_batch.shape))\n",
        "\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorShape([64, 16]), TensorShape([64, 11]))\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHLvSbG9MMpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ae79277-8eca-4421-866e-4f88289945bb"
      },
      "source": [
        "type(units)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCM9N7hxNtSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_np = sample_output.numpy()\n",
        "state_np = sample_hidden.numpy()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcK3dX8UUDJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7015c143-fa66-4a81-dbcd-cf87fb875708"
      },
      "source": [
        "np.linalg.norm(output_np.transpose([1,0,2])[-1] - state_np)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9w1REt4F4aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKa0S-rnF83Z",
        "colab_type": "text"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whct3TQ-F7wY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size: int, \n",
        "               embedding_dim: int, units: int, batch_size: int):\n",
        "    \"\"\"\n",
        "      Initialize the decoder\n",
        "\n",
        "      :param vocab_size: size of the vocabulary, i.e. maximum integer index + 1.\n",
        "      :param embedding_dim: dimension of the dense embedding\n",
        "      :param units: dimensionality of the output space.\n",
        "      :param batch_size: batch size\n",
        "\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.units = units\n",
        "\n",
        "    # initialize embedding layer (from symbols to dense vectors)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "\n",
        "    # initialize recurrent cells\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x: tf.Tensor, hidden: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"\n",
        "      Forward pass\n",
        "\n",
        "      :param x: input tensor of shape (batch_size, max_input_length)\n",
        "      :param hidden: hidden state initialization tensor of shape (batch_size, units)\n",
        "      \n",
        "      :return:\n",
        "        output tensor of shape (batch_size, sequence_length, units)\n",
        "        state tensor of shape (batch_size, units)\n",
        "    \"\"\"\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfPXYXHlHVL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c659a73d-352e-4321-c43f-47a089c8323a"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z47STXVqIc84",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOLcqFrIZdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnAN1bHWH9Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKQ__rMJIloG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # NO passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXcuz2GfI3Vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a550edd-d6d6-4b80-bc09-b44e4587cb11"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.6742\n",
            "Epoch 1 Batch 100 Loss 2.0431\n",
            "Epoch 1 Batch 200 Loss 1.8460\n",
            "Epoch 1 Batch 300 Loss 1.6160\n",
            "Epoch 1 Loss 1.9543\n",
            "Time taken for 1 epoch 30.616573810577393 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5295\n",
            "Epoch 2 Batch 100 Loss 1.3625\n",
            "Epoch 2 Batch 200 Loss 1.3609\n",
            "Epoch 2 Batch 300 Loss 1.3084\n",
            "Epoch 2 Loss 1.3455\n",
            "Time taken for 1 epoch 21.003697156906128 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0701\n",
            "Epoch 3 Batch 100 Loss 0.9792\n",
            "Epoch 3 Batch 200 Loss 0.9908\n",
            "Epoch 3 Batch 300 Loss 1.0387\n",
            "Epoch 3 Loss 1.0093\n",
            "Time taken for 1 epoch 20.7595374584198 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7245\n",
            "Epoch 4 Batch 100 Loss 0.7645\n",
            "Epoch 4 Batch 200 Loss 0.7615\n",
            "Epoch 4 Batch 300 Loss 0.7333\n",
            "Epoch 4 Loss 0.7598\n",
            "Time taken for 1 epoch 21.409523725509644 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5377\n",
            "Epoch 5 Batch 100 Loss 0.5584\n",
            "Epoch 5 Batch 200 Loss 0.5793\n",
            "Epoch 5 Batch 300 Loss 0.5630\n",
            "Epoch 5 Loss 0.5618\n",
            "Time taken for 1 epoch 21.072704553604126 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4124\n",
            "Epoch 6 Batch 100 Loss 0.4024\n",
            "Epoch 6 Batch 200 Loss 0.4593\n",
            "Epoch 6 Batch 300 Loss 0.3982\n",
            "Epoch 6 Loss 0.4093\n",
            "Time taken for 1 epoch 21.548043966293335 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2701\n",
            "Epoch 7 Batch 100 Loss 0.2665\n",
            "Epoch 7 Batch 200 Loss 0.3139\n",
            "Epoch 7 Batch 300 Loss 0.2809\n",
            "Epoch 7 Loss 0.2950\n",
            "Time taken for 1 epoch 21.20237421989441 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1443\n",
            "Epoch 8 Batch 100 Loss 0.2246\n",
            "Epoch 8 Batch 200 Loss 0.2241\n",
            "Epoch 8 Batch 300 Loss 0.1954\n",
            "Epoch 8 Loss 0.2125\n",
            "Time taken for 1 epoch 21.624613046646118 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1412\n",
            "Epoch 9 Batch 100 Loss 0.1284\n",
            "Epoch 9 Batch 200 Loss 0.1281\n",
            "Epoch 9 Batch 300 Loss 0.1615\n",
            "Epoch 9 Loss 0.1546\n",
            "Time taken for 1 epoch 21.274413347244263 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0811\n",
            "Epoch 10 Batch 100 Loss 0.1309\n",
            "Epoch 10 Batch 200 Loss 0.1112\n",
            "Epoch 10 Batch 300 Loss 0.1385\n",
            "Epoch 10 Loss 0.1160\n",
            "Time taken for 1 epoch 21.568514585494995 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt20w95bSfKd",
        "colab_type": "text"
      },
      "source": [
        "# Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRL1bIKqSguu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input,\n",
        "                                      dec_hidden)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    predicted_word = targ_lang_tokenizer.index_word[predicted_id]\n",
        "    result += predicted_word + ' '\n",
        "\n",
        "    if predicted_word == '<end>':\n",
        "      return result.strip(), sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVpmI2R2S4NF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnYaEnQcTD86",
        "colab_type": "text"
      },
      "source": [
        "# Restore checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yetqVYs9TDmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbfe3e61-a5d0-4a5b-e538-1d7690cf4239"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff7005c1518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Rj_3IJTAZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "31609c3e-768d-493c-ed13-875891391ccf"
      },
      "source": [
        "translate(u'Buenos días.')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> buenos dias . <end>\n",
            "Predicted translation: good morning . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrWVSmXnTLzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8e7ae95f-d545-46ab-950b-d9599c7714c9"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s very busy . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IypfTNQBUANE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa0f5e3a-2a94-4ec9-f9a7-b2f7a7cab58f"
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp9blTRYUB95",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f7bf2250-bdc4-4838-a8f3-c9f7d226fad7"
      },
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted translation: are you still at home ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HYxOyaDUES1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "49e5e0fd-0116-46c0-c191-65e082c205a9"
      },
      "source": [
        "translate(u'trata de averiguarlo.')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> trata de averiguarlo . <end>\n",
            "Predicted translation: try to figure it out . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzIKI1XtUGah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f47fe2c9-26f9-43bf-b856-089c7fff6893"
      },
      "source": [
        "type(target_tensor_train)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58eUSSIjUYkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = tf.data.Dataset.from_tensor_slices((input_tensor_val , target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "testset = testset.batch(1, drop_remainder=True)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvmOmHMyVAKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "e301e2e2-04b6-4448-987a-c1b13d016d21"
      },
      "source": [
        " for (batch, (inp, targ)) in enumerate(testset.take(10)):\n",
        "   inp_sentence = ' '.join(inp_lang_tokenizer.sequences_to_texts(inp.numpy())[0].split(' ')[1:-1])\n",
        "   targ_sentence = ' '.join(targ_lang_tokenizer.sequences_to_texts(targ.numpy())[0].split(' ')[1:-1])\n",
        "\n",
        "   predicted_sentence, _ = evaluate(inp_sentence)\n",
        "   predicted_sentence = ' '.join(predicted_sentence.split(' ')[:-1])\n",
        "   print('\\ninput: {}'.format(inp_sentence))\n",
        "   print('target: {}'.format(targ_sentence))\n",
        "   print('predicted: {}'.format(predicted_sentence))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "input: ¿ puedes ir con nosotras ?\n",
            "target: can you go with us ?\n",
            "predicted: can you go with us ?\n",
            "\n",
            "input: nadie lo quiere .\n",
            "target: nobody wants it .\n",
            "predicted: nobody likes her .\n",
            "\n",
            "input: me he mudado .\n",
            "target: i moved .\n",
            "predicted: i ve been shot .\n",
            "\n",
            "input: queremos negociar .\n",
            "target: we want to negotiate .\n",
            "predicted: we want justice .\n",
            "\n",
            "input: estoy hastiada de pescado .\n",
            "target: i m sick of fish .\n",
            "predicted: i m sure of you .\n",
            "\n",
            "input: odio los mosquitos .\n",
            "target: i hate mosquitoes .\n",
            "predicted: i hate raccoons .\n",
            "\n",
            "input: cuenta con eso .\n",
            "target: count on it .\n",
            "predicted: i ll take it .\n",
            "\n",
            "input: reemplazadlo .\n",
            "target: replace it .\n",
            "predicted: make it short .\n",
            "\n",
            "input: tom ama su empleo .\n",
            "target: tom loves his job .\n",
            "predicted: tom loves his work .\n",
            "\n",
            "input: yo no le temo a la muerte .\n",
            "target: i don t fear death .\n",
            "predicted: i fear no music .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN3-nEb7VsNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 135,
      "outputs": []
    }
  ]
}