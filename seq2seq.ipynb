{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMeLwOrN0OZLXKEmqaDPsyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardo1980/colab_bench/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDhDs0QAG8bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygFaK8tzHtTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b102277c-c7ba-46f2-b9da-6aa92250391d"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049Pf0JvHyKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALAwgdDFKnii",
        "colab_type": "text"
      },
      "source": [
        "# Dataset download\n",
        "Resources: http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUjrFBtZhDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGO6Q_ODZs0b",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHh28tNZj3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s: str) -> str:\n",
        "  \"\"\"\n",
        "    Converts unicode string to ascii\n",
        "  \n",
        "    Non-spacing marks (Mn category) are discarded,\n",
        "    see https://www.fileformat.info/info/unicode/category/Mn/list.htm \n",
        "\n",
        "    Applies Normalization Form C (NFC)\n",
        "\n",
        "    :param s: unicode string\n",
        "    :return: ascii string\n",
        "  \"\"\"\n",
        "\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w: str) -> str:\n",
        "  \"\"\"\n",
        "    Convert single string sentence\n",
        "    \n",
        "    1. unicode to ascii\n",
        "    2. adds a space between word and following punctuation\n",
        "    3. removes all chars except a-Z, A-Z, , \".\", \"?\", \"!\", \",\",\"多\"\n",
        "    4. removes leading/trailing blanks\n",
        "    5. adds start/end tokens \n",
        "  \"\"\"\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except basic punctuation and alpha chars\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DAaLVbhaLNY",
        "colab_type": "text"
      },
      "source": [
        "## Test preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLFUh9IZaG4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "514995e8-2b89-4278-ca63-d78df67409a4"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyF2p648aokj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGND9F-baPoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def create_dataset(path: str, num_examples: int) -> List:\n",
        "  \"\"\"\n",
        "    Create pairs of sentences\n",
        "  \n",
        "    1. Remove the accents\n",
        "    2. Clean the sentences\n",
        "    3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "\n",
        "    :param path: path to input file\n",
        "    :param num_examples:\n",
        "    :return: \n",
        "  \"\"\"\n",
        "  # each line contains two columns separated by tab character\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  # split lines, preprocess phrases, get a tuple for each line\n",
        "  sentence_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  # rearrange to a tuple for each language\n",
        "  return zip(*sentence_pairs)\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzFJpEW2a4r0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToWcx0ja4DJ",
        "colab_type": "text"
      },
      "source": [
        "## Test dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ0ASMxta0ob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d1deac40-77a4-4392-a437-278dfc9a162f"
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UphyWTcMOAbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDVGTIFoPkYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}