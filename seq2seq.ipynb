{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuu5zm0DSUcu9Ryv99e7JH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardo1980/colab_bench/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDhDs0QAG8bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygFaK8tzHtTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "112fdc3f-b340-4b93-e73a-ba6cf2d0bd0b"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049Pf0JvHyKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALAwgdDFKnii",
        "colab_type": "text"
      },
      "source": [
        "# Dataset download\n",
        "Resources: http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUjrFBtZhDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81df36f0-5d97-4ac0-c479-9370fc943154"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGO6Q_ODZs0b",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHh28tNZj3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s: str) -> str:\n",
        "  \"\"\"\n",
        "    Converts unicode string to ascii\n",
        "  \n",
        "    Non-spacing marks (Mn category) are discarded,\n",
        "    see https://www.fileformat.info/info/unicode/category/Mn/list.htm \n",
        "\n",
        "    Applies Normalization Form C (NFC)\n",
        "\n",
        "    :param s: unicode string\n",
        "    :return: ascii string\n",
        "  \"\"\"\n",
        "\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w: str) -> str:\n",
        "  \"\"\"\n",
        "    Convert single string sentence\n",
        "    \n",
        "    1. unicode to ascii\n",
        "    2. adds a space between word and following punctuation\n",
        "    3. removes all chars except a-Z, A-Z, , \".\", \"?\", \"!\", \",\",\"多\"\n",
        "    4. removes leading/trailing blanks\n",
        "    5. adds start/end tokens\n",
        "\n",
        "    :param w: unicode string string\n",
        "    :return: cleaned string  \n",
        "  \"\"\"\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except basic punctuation and alpha chars\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DAaLVbhaLNY",
        "colab_type": "text"
      },
      "source": [
        "## Test preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLFUh9IZaG4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bda53d2b-9f16-496a-a752-149b4dda9579"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyF2p648aokj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGND9F-baPoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "def create_dataset(path: str, num_examples: int) -> List[List[str]]:\n",
        "  \"\"\"\n",
        "    Create pairs of sentences\n",
        "  \n",
        "    1. Remove the accents\n",
        "    2. Clean the sentences\n",
        "    3. Return sentences grouped by language]\n",
        "\n",
        "    :param path: path to input file\n",
        "    :param num_examples: maximum number of examples\n",
        "    :return: tuple containing two list of sequences, one for each column in input file \n",
        "  \"\"\"\n",
        "  # each line contains two columns separated by tab character\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  # split lines, preprocess phrases, get a tuple for each line\n",
        "  sentence_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  # rearrange to a tuple for each language\n",
        "  return zip(*sentence_pairs)\n",
        "\n",
        "\n",
        "def tokenize(lang: List[str]) -> Tuple[np.ndarray, tf.keras.preprocessing.text.Tokenizer]:\n",
        "  \"\"\"\n",
        "    Fit a tokenizer on input list of sentences\n",
        "\n",
        "    :param lang: list of sentences of same language\n",
        "    :return: a tuple of:\n",
        "      a tensor of size [NUMBER_OF_SENTENCES, SENTENCE_SIZE] contaning the vectorizations of the sentences\n",
        "      a learned tokenizer\n",
        "  \"\"\"\n",
        "\n",
        "  # create vanilla tokenizer\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  \n",
        "  # learn tokenization procedure on given set of sentences\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  # transforms sentences in sequences of integers (sequences are of different lengths)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  # pad sequenes\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None) -> Tuple[np.ndarray, np.ndarray, tf.keras.preprocessing.text.Tokenizer, tf.keras.preprocessing.text.Tokenizer]:\n",
        "  \"\"\"\n",
        "    Load dataset, with preprocessing and tokenization\n",
        "\n",
        "    :param path: path to input file\n",
        "     :param num_examples: maximum number of examples\n",
        "  \"\"\"\n",
        "\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  # tokenization\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzFJpEW2a4r0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToWcx0ja4DJ",
        "colab_type": "text"
      },
      "source": [
        "## Test dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ0ASMxta0ob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "36531e4a-c3c9-40a4-dab1-52cadaa84cfd"
      },
      "source": [
        "target_sentences, input_sentences = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UphyWTcMOAbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns-I_sQWQg_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "809f243d-bd6f-448e-85e7-f43e85dbf06b"
      },
      "source": [
        "target_sentences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> go . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOABTxqfTgdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72b9e901-3a30-44b1-ef83-d4c55f3b3e52"
      },
      "source": [
        "targ_lang_tokenizer.texts_to_sequences([['go']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[36]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61GBP7nzTuC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a89bea0b-674e-480c-df13-60925645ca25"
      },
      "source": [
        "targ_lang_tokenizer.sequences_to_texts(\n",
        "    [np.arange(20)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> <end> . i tom you ? is a it s t the he to we me m this']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt89-PaVU6wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmaOi5C0U7-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYaiUQSVd1g",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzvsBrm0Vctw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 30000\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "purU06h-VGLH",
        "colab_type": "text"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqXhEg8_U_3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, num_examples)\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnHdHnaVVL1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp1QdXWKWFAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}